{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f1a65c-f4f3-44a1-86fc-f9f13e0c3e08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8094513c-e13d-4e06-b6ea-ca61fc61f6d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ConvertToDataFrame\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e887764f-effe-4e0a-b6eb-9145485cd2f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n",
    " (\"Michael,,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n",
    " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b69e1a96-2901-4d38-901a-3012b7c91670",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: string (nullable = true)\n |-- Languageatschool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- languageatwork: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- Currentstate: string (nullable = true)\n |-- Previousstate: string (nullable = true)\n\n+----------------+------------------+---------------+------------+-------------+\n|            Name|  Languageatschool| languageatwork|Currentstate|Previousstate|\n+----------------+------------------+---------------+------------+-------------+\n|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|\n|  Michael,,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|\n|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|\n+----------------+------------------+---------------+------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Languageatschool\", ArrayType(StringType()),True ),\n",
    "    StructField(\"languageatwork\", ArrayType(StringType()),True),\n",
    "    StructField(\"Currentstate\", StringType(), True),\n",
    "    StructField(\"Previousstate\", StringType(), True)\n",
    "])\n",
    "df_latest = spark.createDataFrame(data,schema)\n",
    "df_latest.printSchema()\n",
    "df_latest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc9357b1-1ca7-4166-9c2e-71daaaa43f84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: string (nullable = true)\n |-- Languageatschool: string (nullable = false)\n |-- languageatwork: string (nullable = false)\n |-- Currentstate: string (nullable = true)\n |-- Previousstate: string (nullable = true)\n\n+----------------+----------------+--------------+------------+-------------+\n|            Name|Languageatschool|languageatwork|Currentstate|Previousstate|\n+----------------+----------------+--------------+------------+-------------+\n|    James,,Smith|  Java,Scala,C++|    Spark,Java|          OH|           CA|\n|  Michael,,Rose,|  Spark,Java,C++|    Spark,Java|          NY|           NJ|\n|Robert,,Williams|       CSharp,VB|  Spark,Python|          UT|           NV|\n+----------------+----------------+--------------+------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "#saving dataframe as csv\n",
    "from pyspark.sql.functions import col, concat_ws\n",
    "df2 = df_latest.withColumn(\"Languageatschool\", concat_ws(\",\", col(\"languageatschool\")))\n",
    "df3 = df2.withColumn(\"languageatwork\", concat_ws(\",\", col(\"languageatwork\")))\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b64e14ad-7616-47c5-9fb8-7197e64a252f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3.write.format(\"csv\").option(\"header\", \"true\").option(\"inferschema\", \"false\").save(\"FileStore/tables/newdata.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8211472-bffe-4934-8bcc-93db3a8a24e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|         nameAsArray|\n+--------------------+\n|    [James, , Smith]|\n| [Michael, , Rose, ]|\n|[Robert, , Williams]|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#1. We need to check how to create array of multiple value of one columns\n",
    "from pyspark.sql.functions import split\n",
    "df3.select(split(df3.Name,\",\").alias(\"nameAsArray\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd293275-42d4-4356-bd78-9c6f9a09b33d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n|            Name|  States|\n+----------------+--------+\n|    James,,Smith|[OH, CA]|\n|  Michael,,Rose,|[NY, NJ]|\n|Robert,,Williams|[UT, NV]|\n+----------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#2.How to create array of two columns value\n",
    "from pyspark.sql.functions import array\n",
    "df3.select(df3.Name,array(df3.Currentstate,df3.Previousstate).alias(\"States\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f66ce8f8-243a-48dd-a219-e4a573d3cfc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+\n|            name|value_contain|\n+----------------+-------------+\n|    James,,Smith|         true|\n|  Michael,,Rose,|         true|\n|Robert,,Williams|        false|\n+----------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "#task3 check whether the column contains  \"Java\" or not and return the value\n",
    "from pyspark.sql.functions import array_contains\n",
    "df.select(df.name,array_contains(df[\"languageatwork\"],\"Java\")\n",
    "    .alias(\"value_contain\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bade0cc1-49c1-4bb7-a71e-1c7afcd95f3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n|  Name|              Skills|\n+------+--------------------+\n|Nitesh|[Pyspark, databri...|\n+------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "data3=[(\"Nitesh\",[\"Pyspark\",\"databricks\",\"Snowflake\",\"DBT\"])]\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Skills\", ArrayType(StringType()), True)])\n",
    "df4=spark.createDataFrame(data3,schema=schema)\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d6fb41-916f-4504-9049-780b95965071",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: string (nullable = true)\n |-- col: string (nullable = true)\n\n+------+----------+\n|  Name|       col|\n+------+----------+\n|Nitesh|   Pyspark|\n|Nitesh|databricks|\n|Nitesh| Snowflake|\n|Nitesh|       DBT|\n+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df5 = df4.select(df4.Name,explode(df4.Skills))\n",
    "df5.printSchema()\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a852efe-129f-4e11-836d-08bc935d0a32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dictionary\n",
    "dataDictionary = [\n",
    "        ('James',{'hair':'black'}),\n",
    "        ('Michael',{'hair':'brown'}),\n",
    "        ('Robert',{'hair':'red'}),\n",
    "        ('Washington',{'hair':'grey'}),\n",
    "        ('Jefferson',{'hair':'brown'})\n",
    "        ]\n",
    "from pyspark.sql.types import StringType, MapType\n",
    "mapCol = MapType(StringType(),StringType(),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12b343df-c767-4fd9-88f9-186beabdfc2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, MapType\n",
    "schema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('Hair', MapType(StringType(),StringType()),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b99ed040-b2e2-4c25-978c-5cde2bf8fede",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- Hair: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+---------------+\n|name      |Hair           |\n+----------+---------------+\n|James     |{hair -> black}|\n|Michael   |{hair -> brown}|\n|Robert    |{hair -> red}  |\n|Washington|{hair -> grey} |\n|Jefferson |{hair -> brown}|\n+----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "dataDictionary = [\n",
    "        ('James',{'hair':'black'}),\n",
    "        ('Michael',{'hair':'brown'}),\n",
    "        ('Robert',{'hair':'red'}),\n",
    "        ('Washington',{'hair':'grey'}),\n",
    "        ('Jefferson',{'hair':'brown'})\n",
    "        ]\n",
    "df = spark.createDataFrame(data=dataDictionary, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a967f4cc-af35-4729-a0fd-77779d45a7cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+\n|      name| key|value|\n+----------+----+-----+\n|     James|hair|black|\n|   Michael|hair|brown|\n|    Robert|hair|  red|\n|Washington|hair| grey|\n| Jefferson|hair|brown|\n+----------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df.select(df.name,explode(df.Hair)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03000840-92aa-4266-935e-542fe59d382c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roshan\n"
     ]
    }
   ],
   "source": [
    " from pyspark.sql import Row\n",
    " row = Row(name='roshan',age=24,salary=10000)\n",
    " print(row.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4167c088-85fa-434d-8ea8-7c00d48ac1f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anish\n5.6\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "row=Row(\"name\",\"height\")\n",
    "person = row('anish','5.6')\n",
    "print(person.name)\n",
    "print(person.height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e15a937-1218-4290-9910-2821fd5a1fcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#data\n",
    "(“James,,Smith\", [\"Java\",\"Scala\",\"C++\"], “CA\"), \n",
    "(“Michael,Rose,\", [\"Spark\",\"Java\",\"C++\"], “NJ\"),\n",
    "(“Robert,,Williams”,[\"CSharp\",\"VB\"] ,”NV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "023470d3-a239-4230-a57f-82405e2b573e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name='James,,Smith', Languageatschool=['Java', 'Scala', 'C++'], state='CA'), Row(name='Michael,Rose,', Languageatschool=['Spark', 'Java', 'C++'], state='NJ'), Row(name='Robert,,Williams', Languageatschool=['CSharp', 'VB'], state='NV')]\n"
     ]
    }
   ],
   "source": [
    "#creating RDD using row\n",
    "from pyspark.sql import SparkSession, Row\n",
    "spark = SparkSession.builder.appName('Row').getOrCreate()\n",
    "\n",
    "data = [Row(name=\"James,,Smith\",Languageatschool=[\"Java\",\"Scala\",\"C++\"],state=\"CA\"), \n",
    "    Row(name=\"Michael,Rose,\",Languageatschool=[\"Spark\",\"Java\",\"C++\"],state=\"NJ\"),\n",
    "    Row(name=\"Robert,,Williams\",Languageatschool=[\"CSharp\",\"VB\"],state=\"NV\")]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16778652-39b4-4222-a5d4-43c8f228b062",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- Languageatschool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- state: string (nullable = true)\n\n+----------------+------------------+-----+\n|            name|  Languageatschool|state|\n+----------------+------------------+-----+\n|    James,,Smith|[Java, Scala, C++]|   CA|\n|   Michael,Rose,|[Spark, Java, C++]|   NJ|\n|Robert,,Williams|      [CSharp, VB]|   NV|\n+----------------+------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#creating dataframe using row\n",
    "df=spark.createDataFrame(data)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "624cb200-7f5c-4805-8349-842bc2bc422c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- languagesAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#changing column name using df functions\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "df=spark.createDataFrame(data).toDF(*columns)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2deba4be-0c42-4645-a486-7c9fe9399a02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+------------+\n|            name| languagesAtSchool|currentState|\n+----------------+------------------+------------+\n|    James,,Smith|[Java, Scala, C++]|          CA|\n|   Michael,Rose,|[Spark, Java, C++]|          NJ|\n|Robert,,Williams|      [CSharp, VB]|          NV|\n+----------------+------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e708b01-fa53-4104-a02f-df7f53750cf6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- country: string (nullable = true)\n |-- state: string (nullable = true)\n\n+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|    James|   Smith|    USA|   CA|\n|  Michael|    Rose|    USA|   NY|\n|   Robert|Williams|    USA|   CA|\n|    Maria|   Jones|    USA|   FL|\n+---------+--------+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df=spark.createDataFrame(data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "211da9f3-f401-49b9-801d-23560ebf0bb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|firstname|\n+---------+\n|    James|\n|  Michael|\n|   Robert|\n|    Maria|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(df.firstname).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7074f219-460d-4d5f-adc9-7acf00de4457",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n|country|\n+-------+\n|    USA|\n|    USA|\n|    USA|\n|    USA|\n+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(df['country']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62d7c205-a3b6-4252-8141-ebc6a3aec45b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('/FileStore/tables/MOCK_DATA-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7cd6edf-d39c-40c6-bfa4-8d0531f3c461",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+--------------------+------+--------------------+------------+----------+----------+---------------+\n|_c0|       _c1|       _c2|                 _c3|   _c4|                 _c5|         _c6|       _c7|       _c8|            _c9|\n+---+----------+----------+--------------------+------+--------------------+------------+----------+----------+---------------+\n| id|first_name| last_name|               email|gender|                city|       phone|      date|  password|confirmpassword|\n|  1|  Gauthier|  Taudevin|gtaudevin0@google.cn|  Male|Municipio de Copa...|456-347-6401| 7/18/2022|K4tDIv1QfJ|       Z9KrC3tr|\n|  2|     Judah|  Sircombe|jsircombe1@chicag...|  Male|           Tapakrejo|575-376-0675|  1/7/2023|    8kdhrq|     ZftH4ElCFf|\n|  3|    Perice|Camilletti|pcamilletti2@bbc....|  Male|            Den Haag|811-772-2893| 3/21/2023| P1IdX5Th5|         zrFfbs|\n|  4|  Elizabet|   Blewmen|eblewmen3@blinkli...|Female|            Laveiras|687-217-6248| 6/26/2022| VYSZQjZir|   3ta8WQvguxtT|\n|  5|  Etheline|    Garret|   egarret4@gmpg.org|Female|            Jandayan|966-407-1091| 2/22/2023|eg0xKEPula|       la9BO0uk|\n|  6|  Ignacius|      Rome| irome5@linkedin.com|  Male|    União da Vitória|333-339-3344|10/20/2022|    q25Y4j|        U5N8Ov8|\n|  7|   Augusta|   Battany|abattany6@csmonit...|Female|          Yasynuvata|990-942-4385| 5/17/2022| ML0djjsLX|        pKarNaa|\n|  8|      Elle| Pepperell|epepperell7@japan...|Female|  Krasnyye Barrikady|410-874-5108|  1/8/2023|  13okYRnN|     3LmYrbhF7z|\n|  9| Mackenzie|    Kohter|mkohter8@google.c...|  Male|              Sokal’|997-180-2269| 11/9/2022|ADWKVqKwkm|    iioeC6WdiBF|\n| 10|   Tiertza|   Redsall|tredsall9@netlog.com|Female|           Sawaengha|371-817-5053| 1/27/2023|  o5qEIflS|      bONwhuhPj|\n+---+----------+----------+--------------------+------+--------------------+------------+----------+----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fcfb874-aaec-4d18-b4b3-1f47c7ed9fd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+------+--------------------+------------+\n|       _c1|       _c2|                 _c3|   _c4|                 _c5|         _c6|\n+----------+----------+--------------------+------+--------------------+------------+\n|first_name| last_name|               email|gender|                city|       phone|\n|  Gauthier|  Taudevin|gtaudevin0@google.cn|  Male|Municipio de Copa...|456-347-6401|\n|     Judah|  Sircombe|jsircombe1@chicag...|  Male|           Tapakrejo|575-376-0675|\n|    Perice|Camilletti|pcamilletti2@bbc....|  Male|            Den Haag|811-772-2893|\n|  Elizabet|   Blewmen|eblewmen3@blinkli...|Female|            Laveiras|687-217-6248|\n|  Etheline|    Garret|   egarret4@gmpg.org|Female|            Jandayan|966-407-1091|\n+----------+----------+--------------------+------+--------------------+------------+\nonly showing top 6 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.select(df.columns[1:7]).show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9a2d428-1880-4888-a434-dd19b3394ce8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+------+\n|year|month| title|rating|\n+----+-----+------+------+\n|2012|    8|Batman|   9.8|\n|2012|    8|  Hero|   8.7|\n|2012|    7| Robot|   5.5|\n|2011|    7|   git|   2.0|\n+----+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data2 = [(2012,8,\"Batman\",9.8),\n",
    "           (2012,8,\"Hero\",8.7),\n",
    "           (2012,7,\"Robot\",5.5),\n",
    "           (2011,7,\"git\",2.0)\n",
    "  ]\n",
    "columns = [\"year\",\"month\",\"title\",\"rating\"]\n",
    "df=spark.createDataFrame(data2,schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df4f1f49-7497-48f0-8a96-c86368ef40ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"year\") \\\n",
    "        .parquet(\"/FileStore/tables/yearByPartitions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5078705-d1ec-4259-8c2b-35e20b8b20cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.partitionBy(\"year\") \\\n",
    "        .format(\"avro\").save(\"/FileStore/tables/person_partition1.avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c03c7327-45b3-4960-9e1e-4889d398c24e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"year\",\"month\") \\\n",
    "        .csv(\"/FileStore/tables/person_partition1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "491a7b2d-c468-4f3c-8210-72106e10565f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Spark_Session",
   "notebookOrigID": 876351177773477,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
