{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3df75b41-7bf8-422a-bb92-b4c59e3f2442",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark1 = SparkSession.builder.master(\"local[1]\")\\\n",
    "                            .appName('roshan')\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4e29edf-d684-44cc-a7e9-ea4958e330c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f8b079c4730>\n"
     ]
    }
   ],
   "source": [
    "print(spark1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf174f6-7fee-4b77-810e-d3609ac39f5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark2 = SparkSession.newSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f200148-1f09-4afb-801d-f1eafb12b926",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function SparkSession.newSession at 0x7f8b04539b80>\n"
     ]
    }
   ],
   "source": [
    "print(spark2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52640ae-e00b-499a-9029-4882cc879fbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark3 = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f9c0b9-eeec-4522-ba3b-054f3ac8711e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f8b079c4730>\n"
     ]
    }
   ],
   "source": [
    "print(spark3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91f28a8a-f480-4256-be81-cc411920a2b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partitions = spark3.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e3bad57-99b6-4ba3-abc6-9f4712be8ac6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b2b785-2240-484a-a5ef-c021cfba08b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark4 = SparkSession.builder.master(\"local[1]\")\\\n",
    "                            .appName('roshan-app')\\\n",
    "                            .config(\"spark.sql.shuffle.partitions\",\"2\")\\\n",
    "                            .getOrCreate()                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "844f14f4-7a6d-41c5-b27e-9e30235f9a8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "partitions = spark3.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc99dd3-e784-49c4-8a76-fcb3d12eec8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roshan-app\n"
     ]
    }
   ],
   "source": [
    "app_name = spark4.conf.get(\"spark.app.name\")\n",
    "print(app_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "387655b7-8bf9-4694-a060-87e05845d23d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[30]: [1, 'java', 'hadoop', True, 'akka']"
     ]
    }
   ],
   "source": [
    "words_new = spark4.sparkContext.broadcast([1, \"java\", \"hadoop\", True, \"akka\"])\n",
    "words_new.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc96588f-f269-44c2-8247-1bfa55c61201",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 'roshan', 'java', 'hadoop', True, 'akka']\n"
     ]
    }
   ],
   "source": [
    "words_new.value.insert(1,\"roshan\")\n",
    "print(words_new.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac26c19f-7a8b-4492-bcd6-6a7cd70dd216",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['karki', 'roshan', 'roshan', 'java', 'hadoop', True, 'akka']\n"
     ]
    }
   ],
   "source": [
    "words_new.value[0] = 'karki'\n",
    "print(words_new.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeb2101d-a976-477a-9651-2af21e01114e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['roshan', 'java', 'hadoop', True, 'akka']\n"
     ]
    }
   ],
   "source": [
    "words_new.value.pop(0)\n",
    "print(words_new.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e91e6371-7441-457a-8cb2-4f665c02635f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': ['anish', 'aastha'], 'age': [24, 25]}\n"
     ]
    }
   ],
   "source": [
    "friends = {'name':[\"anish\",\"aastha\"],'age':[24,25]}\n",
    "friends_info=sc.broadcast(friends)\n",
    "print(friends_info.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b209828-2ae3-40c0-95f9-b53264714db6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "num = sc.accumulator(12)\n",
    "print(num.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7792df9-b54e-476d-99fe-2777061d02ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.5\n"
     ]
    }
   ],
   "source": [
    "num.add(2)\n",
    "num.add(-3)\n",
    "num.add(num.value*2)\n",
    "num.add(num.value/2)\n",
    "print(num.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf5b44a3-9906-453a-bb7e-9a8e792fdeb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " friends.value\n",
    "\n",
    "words_new = spark4.sparkContext.broadcast([1, \"java\", \"hadoop\", True, \"akka\"])\n",
    "words_new.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8678a23a-e718-4470-ad2d-b9123efb0c6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final value is: 150\n"
     ]
    }
   ],
   "source": [
    "# create an accumulator variable\n",
    "num = spark4.sparkContext.accumulator(10)\n",
    "\n",
    "# define a function that uses the accumulator variable\n",
    "def f(x):\n",
    "    global num\n",
    "    num += x\n",
    "\n",
    "# create an RDD from a list of integers\n",
    "rdd = spark4.sparkContext.parallelize([20, 30, 40, 50])\n",
    "\n",
    "# apply the function that uses the accumulator variable to each element of the RDD\n",
    "rdd.foreach(f)\n",
    "\n",
    "# get the value of the accumulator variable\n",
    "final = num.value\n",
    "\n",
    "# print the result\n",
    "print(\"The final value is:\", final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef1803e4-2b2c-4eda-a68d-ff2ef3a78563",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[61]: ParallelCollectionRDD[27] at readRDDFromInputStream at PythonRDD.scala:435"
     ]
    }
   ],
   "source": [
    "data = [1,2,3,4,5]\n",
    "rdd1 = spark.sparkContext.parallelize(data)\n",
    "rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a21c29c-f4f0-4f75-a1d3-1222cd7a3192",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[62]: 4"
     ]
    }
   ],
   "source": [
    "rdd1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3536e1df-5cb4-4d98-849b-322cf4061d90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[64]: 3"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.parallelize(data,3)\n",
    "rdd1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a457620-1f06-45f9-aaa6-ce1dccdba64a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alice', 25, 'Engineer'), ('Bob', 30, 'Manager')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "person_data = [\n",
    "    (\"Alice\", 25, \"Engineer\"),\n",
    "    (\"Bob\", 30, \"Manager\"),\n",
    "]\n",
    "\n",
    "person_rdd = spark.sparkContext.parallelize(person_data)\n",
    "\n",
    "print(person_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372cd5b6-6991-40a8-86c4-38716d1e5956",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Alice', 25, 'Engineer')\n"
     ]
    }
   ],
   "source": [
    "first_entry =person_rdd.first()\n",
    "print(first_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c86094a-9972-4728-8570-554b489aee67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text = \"1\\n2\\n3\\n4\\n5\\n\"\n",
    "with open(\"/dbfs/Text.txt\", \"w\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a4b553-b336-43d0-838d-ecd17677fbfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "numbers_rdd = sc.textFile(\"Text.txt\")\n",
    "numbers_rdd = numbers_rdd.map(lambda x: int(x))\n",
    "print(numbers_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93eece78-36fd-4c60-a93d-bda019c7a5bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_spark_session",
   "notebookOrigID": 1100333016920406,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
