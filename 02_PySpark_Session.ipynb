{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cef177a-628d-43d4-9efc-9d084af6c7b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text = \"1\\n2\\n3\\n4\\n5\\n\"\n",
    "with open(\"/dbfs/Text.txt\", \"w\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01abcc29-27db-4d4f-ad37-bb89694ee710",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dbfs:/Text.txt', '1\\n2\\n3\\n4\\n5\\n')]\n['1', '2', '3', '4', '5']\n1\n5\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "rdd1 = spark.sparkContext.wholeTextFiles(\"Text.txt\",6)\n",
    "rdd2 = spark.sparkContext.textFile(\"Text.txt\",4)\n",
    "\n",
    "print(rdd1.collect())\n",
    "print(rdd2.collect())\n",
    "\n",
    "print(rdd1.getNumPartitions())\n",
    "print(rdd2.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e329daf2-2978-4f8c-8f3f-4ab3dad0b9e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "rdd3 = rdd2.repartition(10)\n",
    "print(rdd3.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a5acda-9355-4c1e-a881-bb3ac2fd9af1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# print(rdd2.getNumPartitions())\n",
    "rdd4=rdd2.coalesce(4)\n",
    "print(rdd4.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f593c97-710f-4199-bdbf-a20719e02fd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "friends = {'name':[\"anish\",\"aastha\"],'age':[24,25]}\n",
    "friend = spark.SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7fe45d4-385d-42ea-8917-8f0ad003b160",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data1 = ([\"aastha\",12,\"gaighat\"],[\"roshan\",15,\"pokhara\"],[\"anish\",20,\"dhang\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd57fc83-d4ba-442d-b3c0-6a4a7ae15538",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_1</th><th>_2</th><th>_3</th></tr></thead><tbody><tr><td>aastha</td><td>12</td><td>gaighat</td></tr><tr><td>roshan</td><td>15</td><td>pokhara</td></tr><tr><td>anish</td><td>20</td><td>dhang</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "aastha",
         12,
         "gaighat"
        ],
        [
         "roshan",
         15,
         "pokhara"
        ],
        [
         "anish",
         20,
         "dhang"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "_3",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(data=data1)\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30f92418-1529-4336-aa44-41eeaee8e258",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>first_name</th><th>age</th><th>location</th></tr></thead><tbody><tr><td>aastha</td><td>12</td><td>gaighat</td></tr><tr><td>roshan</td><td>15</td><td>pokhara</td></tr><tr><td>anish</td><td>20</td><td>dhang</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "aastha",
         12,
         "gaighat"
        ],
        [
         "roshan",
         15,
         "pokhara"
        ],
        [
         "anish",
         20,
         "dhang"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "first_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns =[\"first_name\",\"age\",\"location\"]\n",
    "df2 = spark.createDataFrame(data=data1,schema=columns)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "147cbb8a-4535-4918-84fa-008efb5fd317",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n| id|value|\n+---+-----+\n|  1|  one|\n|  2|  two|\n|  3|three|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD of tuples\n",
    "data = sc.parallelize([(1, \"one\"), (2, \"two\"), (3, \"three\")])\n",
    "\n",
    "# Convert the RDD to a DataFrame\n",
    "df = data.toDF([\"id\", \"value\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68f6e6e0-c1b3-41a8-b6e3-fdc25af55272",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+\n| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n+---+-------------+------------+-------------+------------+-----------+\n|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n|  6|          5.4|         3.9|          1.7|         0.4|Iris-setosa|\n|  7|          4.6|         3.4|          1.4|         0.3|Iris-setosa|\n|  8|          5.0|         3.4|          1.5|         0.2|Iris-setosa|\n|  9|          4.4|         2.9|          1.4|         0.2|Iris-setosa|\n| 10|          4.9|         3.1|          1.5|         0.1|Iris-setosa|\n| 11|          5.4|         3.7|          1.5|         0.2|Iris-setosa|\n| 12|          4.8|         3.4|          1.6|         0.2|Iris-setosa|\n| 13|          4.8|         3.0|          1.4|         0.1|Iris-setosa|\n| 14|          4.3|         3.0|          1.1|         0.1|Iris-setosa|\n| 15|          5.8|         4.0|          1.2|         0.2|Iris-setosa|\n| 16|          5.7|         4.4|          1.5|         0.4|Iris-setosa|\n| 17|          5.4|         3.9|          1.3|         0.4|Iris-setosa|\n| 18|          5.1|         3.5|          1.4|         0.3|Iris-setosa|\n| 19|          5.7|         3.8|          1.7|         0.3|Iris-setosa|\n| 20|          5.1|         3.8|          1.5|         0.3|Iris-setosa|\n+---+-------------+------------+-------------+------------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/FileStore/tables/Iris-1.csv\", header=True)\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f96a69a-2690-4b65-87fe-062ec0ba586e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----+\n|age|         city|name|\n+---+-------------+----+\n| 30|     New York|John|\n| 25|San Francisco|Mary|\n| 40|  Los Angeles| Bob|\n+---+-------------+----+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"/FileStore/tables/dummydata-2.json\")\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b3f967-6226-4dc0-8be4-8c577b028e3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": {
        "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
        "text/plain": ""
       },
       "datasetInfos": [],
       "executionCount": null,
       "metadata": {
        "kernelSessionId": "f6a9687c-56886db13f5f63f7a9d8b1d6"
       },
       "removedWidgets": [],
       "type": "mimeBundle"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f561990-fa1f-4b09-8fe0-eb4195d005f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[22]: pyspark.rdd.RDD"
     ]
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "161f4868-04dc-4378-9d22-621815fa6540",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>city</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>New York</td>\n",
       "      <td>John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>Mary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Bob</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>city</th>\n      <th>name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30</td>\n      <td>New York</td>\n      <td>John</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>25</td>\n      <td>San Francisco</td>\n      <td>Mary</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>40</td>\n      <td>Los Angeles</td>\n      <td>Bob</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_pd =df.toPandas()\n",
    "data_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17e88908-9eb2-4f19-942b-8bd405392cbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[23]: pandas.core.frame.DataFrame"
     ]
    }
   ],
   "source": [
    "type(data_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "152b6ad2-e531-43f4-9495-3e583115c449",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task- Create Dataframe and write this to parquet file\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Write to Parquet\").getOrCreate()\n",
    "\n",
    "# create a sample DataFrame\n",
    "data = [(\"Roshan\", 15), (\"Aastha\", 13), (\"Anish\", 18)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "\n",
    "# write DataFrame to Parquet format\n",
    "df.write.format(\"parquet\").save(\"/path/to/parque_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "172ab534-6b94-4dcd-9a19-209bcbff12d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n|  name|age|\n+------+---+\n|Roshan| 15|\n|Aastha| 13|\n| Anish| 18|\n+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "#Task- Read the Parquet file, Append the data , Overwrite the data in parquet file\n",
    "\n",
    "par_df=spark.read.parquet(\"/path/to/parque_file\")\n",
    "par_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31d69d53-c7c3-4636-b4d0-bcb5319557e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_data = [('Alice', 28), ('Bob', 32)]\n",
    "new_df = spark.createDataFrame(new_data)\n",
    "df = df.union(new_df)\n",
    "df.write.mode('append').parquet(\"/path/to/parque_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fa09488-c9af-411a-bd3b-778875620431",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n|  name|age|\n+------+---+\n|Roshan| 15|\n|Aastha| 13|\n| Anish| 18|\n| Alice| 28|\n|   Bob| 32|\n+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08a30987-7c3c-4631-a991-37f497523e9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.union(new_df).write.format(\"parquet\").mode(\"overwrite\").save(\"/FileStore/tables/userdata.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "619016a0-01d5-44d0-a6bb-99e601c0023e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n|  name|age|\n+------+---+\n|Roshan| 15|\n|Aastha| 13|\n| Anish| 18|\n| Alice| 28|\n|   Bob| 32|\n+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "253df95f-0f15-4f39-aac0-a163aa62ac4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>firstname</th><th>middlename</th><th>lastname</th><th>id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>James</td><td></td><td>Smith</td><td>36636</td><td>M</td><td>3000</td></tr><tr><td>Michael</td><td>Rose</td><td></td><td>40288</td><td>M</td><td>4000</td></tr><tr><td>Robert</td><td></td><td>Williams</td><td>42114</td><td>M</td><td>4000</td></tr><tr><td>Maria</td><td>Anne</td><td>Jones</td><td>39192</td><td>F</td><td>4000</td></tr><tr><td>Jen</td><td>Mary</td><td>Brown</td><td></td><td>F</td><td>-1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "James",
         "",
         "Smith",
         "36636",
         "M",
         3000
        ],
        [
         "Michael",
         "Rose",
         "",
         "40288",
         "M",
         4000
        ],
        [
         "Robert",
         "",
         "Williams",
         "42114",
         "M",
         4000
        ],
        [
         "Maria",
         "Anne",
         "Jones",
         "39192",
         "F",
         4000
        ],
        [
         "Jen",
         "Mary",
         "Brown",
         "",
         "F",
         -1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "firstname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "middlename",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lastname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#datypes predefined\n",
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    " \n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\", StringType(), True),\n",
    "    StructField(\"middlename\", StringType(), True),\n",
    "    StructField(\"lastname\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "df = spark.createDataFrame(data, schema)        \n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "483a901b-5361-45c9-b36f-a41038717254",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[69]: [('firstname', 'string'),\n ('middlename', 'string'),\n ('lastname', 'string'),\n ('id', 'string'),\n ('gender', 'string'),\n ('salary', 'int')]"
     ]
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c7dd76e-9ebd-45e2-9133-8ffdc80961a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- SepalLengthCm: string (nullable = true)\n |-- SepalWidthCm: string (nullable = true)\n |-- PetalLengthCm: string (nullable = true)\n |-- PetalWidthCm: integer (nullable = true)\n |-- Species: integer (nullable = true)\n\n+---+-------------+------------+-------------+------------+-------+\n| id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|Species|\n+---+-------------+------------+-------------+------------+-------+\n|  1|          5.1|         3.5|          1.4|        null|   null|\n|  2|          4.9|         3.0|          1.4|        null|   null|\n|  3|          4.7|         3.2|          1.3|        null|   null|\n|  4|          4.6|         3.1|          1.5|        null|   null|\n|  5|          5.0|         3.6|          1.4|        null|   null|\n|  6|          5.4|         3.9|          1.7|        null|   null|\n|  7|          4.6|         3.4|          1.4|        null|   null|\n|  8|          5.0|         3.4|          1.5|        null|   null|\n|  9|          4.4|         2.9|          1.4|        null|   null|\n| 10|          4.9|         3.1|          1.5|        null|   null|\n| 11|          5.4|         3.7|          1.5|        null|   null|\n| 12|          4.8|         3.4|          1.6|        null|   null|\n| 13|          4.8|         3.0|          1.4|        null|   null|\n| 14|          4.3|         3.0|          1.1|        null|   null|\n| 15|          5.8|         4.0|          1.2|        null|   null|\n| 16|          5.7|         4.4|          1.5|        null|   null|\n| 17|          5.4|         3.9|          1.3|        null|   null|\n| 18|          5.1|         3.5|          1.4|        null|   null|\n| 19|          5.7|         3.8|          1.7|        null|   null|\n| 20|          5.1|         3.8|          1.5|        null|   null|\n+---+-------------+------------+-------------+------------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#StructType is used to define the schema of a DataFrame as a whole whereas StructField is used to define individual columns of the schema.\n",
    "#The header parameter is set to True to indicate that the first row of the CSV file contains the column names.\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"SepalLengthCm\", StringType(), True),\n",
    "    StructField(\"SepalWidthCm\", StringType(), False),\n",
    "    StructField(\"PetalLengthCm\", StringType(), True),\n",
    "    StructField(\"PetalWidthCm\", IntegerType(), True),\n",
    "    StructField(\"Species\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.read.csv(\"/FileStore/tables/Iris.csv\", header=True, schema=schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4bf4a7f-06dc-4d4d-a8ee-bc4ec4df6ffc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Id: string (nullable = true)\n |-- SepalLengthCm: string (nullable = true)\n |-- SepalWidthCm: string (nullable = true)\n |-- PetalLengthCm: string (nullable = true)\n |-- PetalWidthCm: string (nullable = true)\n |-- Species: string (nullable = true)\n\n+---+-------------+------------+-------------+------------+-----------+\n| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n+---+-------------+------------+-------------+------------+-----------+\n|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n|  6|          5.4|         3.9|          1.7|         0.4|Iris-setosa|\n|  7|          4.6|         3.4|          1.4|         0.3|Iris-setosa|\n|  8|          5.0|         3.4|          1.5|         0.2|Iris-setosa|\n|  9|          4.4|         2.9|          1.4|         0.2|Iris-setosa|\n| 10|          4.9|         3.1|          1.5|         0.1|Iris-setosa|\n| 11|          5.4|         3.7|          1.5|         0.2|Iris-setosa|\n| 12|          4.8|         3.4|          1.6|         0.2|Iris-setosa|\n| 13|          4.8|         3.0|          1.4|         0.1|Iris-setosa|\n| 14|          4.3|         3.0|          1.1|         0.1|Iris-setosa|\n| 15|          5.8|         4.0|          1.2|         0.2|Iris-setosa|\n| 16|          5.7|         4.4|          1.5|         0.4|Iris-setosa|\n| 17|          5.4|         3.9|          1.3|         0.4|Iris-setosa|\n| 18|          5.1|         3.5|          1.4|         0.3|Iris-setosa|\n| 19|          5.7|         3.8|          1.7|         0.3|Iris-setosa|\n| 20|          5.1|         3.8|          1.5|         0.3|Iris-setosa|\n+---+-------------+------------+-------------+------------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/FileStore/tables/Iris.csv\", header=True)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc8dac49-cf45-4873-9aab-b3df35eb48ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "\n",
    "Name – First Name,Middle ,Last name\n",
    "Id\n",
    "Gender\n",
    "Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1987050-16d2-4478-8e5d-fdbad96426dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- first_name: string (nullable = true)\n |    |-- middle_name: string (nullable = true)\n |    |-- last_name: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+--------------------+-----+------+------+\n|                name|   id|gender|salary|\n+--------------------+-----+------+------+\n|    {James, , Smith}|36636|     M|  3100|\n|   {Michael, Rose, }|40288|     M|  4300|\n|{Robert, , Williams}|42114|     M|  1400|\n|{Maria, Anne, Jones}|39192|     F|  5500|\n|  {Jen, Mary, Brown}|     |     F|    -1|\n+--------------------+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#dataset\n",
    "data=[((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)]\n",
    "# Define the schema for the input data\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StructType([\n",
    "        StructField(\"first_name\", StringType(), True),\n",
    "        StructField(\"middle_name\", StringType(), True),\n",
    "        StructField(\"last_name\", StringType(), True),\n",
    "    ]), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "df_latest = spark.createDataFrame(data,schema)\n",
    "df_latest.printSchema()\n",
    "df_latest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "170645f5-c813-449a-b322-be5f1f0eb4f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Spark_Session",
   "notebookOrigID": 738265764730115,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
