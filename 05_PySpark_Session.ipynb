{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRWeFTigvH3r",
        "outputId": "3fec3343-d8eb-41e4-de11-58e20d34d673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.9/dist-packages (3.4.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"Replace\").getOrCreate()"
      ],
      "metadata": {
        "id": "vfG8pqivvMn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
        "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
        "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
        "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
        "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
        "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
        "  ]\n",
        "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "simple_df = spark.createDataFrame(simpleData,  schema)\n",
        "simple_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xVzvk2CwBqu",
        "outputId": "f2f08111-5d74-42ba-e6f8-3f9ab07418cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
            "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
            "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
            "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
            "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
            "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We need to Group by Department And State\n",
        "#Output- Sum of salary and bonus\n",
        "from pyspark.sql.functions import col\n",
        "salaryempSalary = simple_df.groupBy('department','state').agg({'salary':'sum','bonus':'sum'})\n",
        "salaryempSalary.withColumn('Total Sum',col('sum(bonus)')+col('sum(salary)')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-a8YYNfwTiH",
        "outputId": "ddd5b93e-f2d3-4255-b0e7-26a3c53ce911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+-----------+---------+\n",
            "|department|state|sum(bonus)|sum(salary)|Total Sum|\n",
            "+----------+-----+----------+-----------+---------+\n",
            "|   Finance|   NY|     34000|     162000|   196000|\n",
            "| Marketing|   NY|     21000|      91000|   112000|\n",
            "|     Sales|   CA|     23000|      81000|   104000|\n",
            "| Marketing|   CA|     18000|      80000|    98000|\n",
            "|   Finance|   CA|     47000|     189000|   236000|\n",
            "|     Sales|   NY|     30000|     176000|   206000|\n",
            "+----------+-----+----------+-----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 3- Group by department and Print the sum of salary, average of salary, minimum salary and maximim salary\n",
        "from pyspark.sql.functions import sum,avg,min,max\n",
        "final_df = simple_df.groupBy('department').agg(sum(\"salary\").alias(\"totalsalary\"), avg(\"salary\").alias(\"avgSalary\"),min(\"salary\").alias(\"minsalary\"),max(\"salary\").alias(\"maxSalary\"))\n"
      ],
      "metadata": {
        "id": "yogDvWXtxEC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg4fmTFrzI9N",
        "outputId": "404d0b18-fe48-4db3-dc4b-f66d7d0ac071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------------+---------+---------+\n",
            "|department|totalsalary|        avgSalary|minsalary|maxSalary|\n",
            "+----------+-----------+-----------------+---------+---------+\n",
            "|     Sales|     257000|85666.66666666667|    81000|    90000|\n",
            "|   Finance|     351000|          87750.0|    79000|    99000|\n",
            "| Marketing|     171000|          85500.0|    80000|    91000|\n",
            "+----------+-----------+-----------------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Taking the files from web\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "from pyspark import SparkFiles\n",
        "spark.sparkContext.addFile(url)"
      ],
      "metadata": {
        "id": "V-2fX1yLzKwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reading the file to create the dataframe\n",
        "df = spark.read.csv(\"file://\"+SparkFiles.get(\"iris.data\"), inferSchema= True)"
      ],
      "metadata": {
        "id": "326974fg0uJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#priting the column and showing the information\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CXb99U_0yF0",
        "outputId": "b2bbb5c5-ab9a-4fd5-f4b8-4350778288e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: double (nullable = true)\n",
            " |-- _c1: double (nullable = true)\n",
            " |-- _c2: double (nullable = true)\n",
            " |-- _c3: double (nullable = true)\n",
            " |-- _c4: string (nullable = true)\n",
            "\n",
            "+---+---+---+---+-----------+\n",
            "|_c0|_c1|_c2|_c3|        _c4|\n",
            "+---+---+---+---+-----------+\n",
            "|5.1|3.5|1.4|0.2|Iris-setosa|\n",
            "|4.9|3.0|1.4|0.2|Iris-setosa|\n",
            "|4.7|3.2|1.3|0.2|Iris-setosa|\n",
            "|4.6|3.1|1.5|0.2|Iris-setosa|\n",
            "|5.0|3.6|1.4|0.2|Iris-setosa|\n",
            "|5.4|3.9|1.7|0.4|Iris-setosa|\n",
            "|4.6|3.4|1.4|0.3|Iris-setosa|\n",
            "|5.0|3.4|1.5|0.2|Iris-setosa|\n",
            "|4.4|2.9|1.4|0.2|Iris-setosa|\n",
            "|4.9|3.1|1.5|0.1|Iris-setosa|\n",
            "|5.4|3.7|1.5|0.2|Iris-setosa|\n",
            "|4.8|3.4|1.6|0.2|Iris-setosa|\n",
            "|4.8|3.0|1.4|0.1|Iris-setosa|\n",
            "|4.3|3.0|1.1|0.1|Iris-setosa|\n",
            "|5.8|4.0|1.2|0.2|Iris-setosa|\n",
            "|5.7|4.4|1.5|0.4|Iris-setosa|\n",
            "|5.4|3.9|1.3|0.4|Iris-setosa|\n",
            "|5.1|3.5|1.4|0.3|Iris-setosa|\n",
            "|5.7|3.8|1.7|0.3|Iris-setosa|\n",
            "|5.1|3.8|1.5|0.3|Iris-setosa|\n",
            "+---+---+---+---+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#renaming the column name\n",
        "iris_df = df.withColumnRenamed('_c0', 'sepal_length') \\\n",
        "                    .withColumnRenamed('_c1', 'sepal_width') \\\n",
        "                    .withColumnRenamed('_c2', 'petal_length') \\\n",
        "                    .withColumnRenamed('_c3', 'petal_width') \\\n",
        "                    .withColumnRenamed('_c4', 'species')\n",
        "iris_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSnePsZO04iH",
        "outputId": "b4722c7f-0091-47c8-b971-12e1353baa48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-----------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|    species|\n",
            "+------------+-----------+------------+-----------+-----------+\n",
            "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n",
            "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|\n",
            "|         5.4|        3.9|         1.7|        0.4|Iris-setosa|\n",
            "|         4.6|        3.4|         1.4|        0.3|Iris-setosa|\n",
            "|         5.0|        3.4|         1.5|        0.2|Iris-setosa|\n",
            "|         4.4|        2.9|         1.4|        0.2|Iris-setosa|\n",
            "|         4.9|        3.1|         1.5|        0.1|Iris-setosa|\n",
            "|         5.4|        3.7|         1.5|        0.2|Iris-setosa|\n",
            "|         4.8|        3.4|         1.6|        0.2|Iris-setosa|\n",
            "|         4.8|        3.0|         1.4|        0.1|Iris-setosa|\n",
            "|         4.3|        3.0|         1.1|        0.1|Iris-setosa|\n",
            "|         5.8|        4.0|         1.2|        0.2|Iris-setosa|\n",
            "|         5.7|        4.4|         1.5|        0.4|Iris-setosa|\n",
            "|         5.4|        3.9|         1.3|        0.4|Iris-setosa|\n",
            "|         5.1|        3.5|         1.4|        0.3|Iris-setosa|\n",
            "|         5.7|        3.8|         1.7|        0.3|Iris-setosa|\n",
            "|         5.1|        3.8|         1.5|        0.3|Iris-setosa|\n",
            "+------------+-----------+------------+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_sepal_width = iris_df.agg(max(\"sepal_width\")).collect()[0][0]\n",
        "print(\"Maximum sepal width\", max_sepal_width)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nLW37dp2Law",
        "outputId": "8ab49ffa-79df-4a0c-c8fa-97155699c2c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sepal width 4.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_petal_width = iris_df.agg(max(\"petal_width\")).collect()[0][0]\n",
        "print(\"Maximum sepal width\", max_petal_width)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XcDKoGE4j7u",
        "outputId": "a117c401-a180-4fd3-c256-61ab3222cdbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sepal width 2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_sepal_length = iris_df.agg(max(\"sepal_length\")).collect()[0][0]\n",
        "print(\"Maximum sepal length\", max_sepal_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRBShrud4olt",
        "outputId": "1474ac77-4a51-4156-e5bf-36d6626e7dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sepal length 7.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_petal_length = iris_df.agg(max(\"petal_length\")).collect()[0][0]\n",
        "print(\"Maximum sepal length\", max_petal_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0OlkndD6Mda",
        "outputId": "0f6d3774-7780-4f27-b12f-ebc5586d00c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sepal length 6.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if max_sepal_width > max_petal_width:\n",
        "    print(\"Sepal width has the maximum value\",max_sepal_width)\n",
        "else:\n",
        "    print(\"Petal width has the maximum value\",max_petal_width)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jStv9WrJ6aph",
        "outputId": "7855b60a-3033-45ed-c0ef-f4c58cb57afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sepal width has the maximum value 4.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if max_sepal_length > max_petal_length:\n",
        "    print(\"Sepal length has the maximum value\",max_sepal_length)\n",
        "else:\n",
        "    print(\"Petal length has the maximum value\",max_petal_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcLHDneZ6tsw",
        "outputId": "5b6db1a0-7f76-42a3-d32e-b664264b6e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sepal length has the maximum value 7.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a PySpark DataFrame from the data\n",
        "data = [('James','','Smith','1991-04-01','M',3000),\n",
        "  ('Michael','Rose','','2000-05-19','M',4000),\n",
        "  ('Robert','','Williams','1978-09-05','M',4000),\n",
        "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
        "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
        "]\n",
        "columns = [\"first_name\", \"middle_name\", \"last_name\", \"dob\", \"gender\", \"salary\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# showing the PySpark DataFrame\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2CYqdiY7BTr",
        "outputId": "83e217fb-59bc-4e52-f102-8ab239db46da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- first_name: string (nullable = true)\n",
            " |-- middle_name: string (nullable = true)\n",
            " |-- last_name: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+----------+-----------+---------+----------+------+------+\n",
            "|first_name|middle_name|last_name|       dob|gender|salary|\n",
            "+----------+-----------+---------+----------+------+------+\n",
            "|     James|           |    Smith|1991-04-01|     M|  3000|\n",
            "|   Michael|       Rose|         |2000-05-19|     M|  4000|\n",
            "|    Robert|           | Williams|1978-09-05|     M|  4000|\n",
            "|     Maria|       Anne|    Jones|1967-12-01|     F|  4000|\n",
            "|       Jen|       Mary|    Brown|1980-02-17|     F|    -1|\n",
            "+----------+-----------+---------+----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"salary\", col(\"salary\").cast(\"integer\"))\n"
      ],
      "metadata": {
        "id": "_4DG7Ko-FIpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGAwn8ZmFfqx",
        "outputId": "23cfda28-9160-46f2-8ca2-40aae73a118d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- first_name: string (nullable = true)\n",
            " |-- middle_name: string (nullable = true)\n",
            " |-- last_name: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "+----------+-----------+---------+----------+------+------+\n",
            "|first_name|middle_name|last_name|       dob|gender|salary|\n",
            "+----------+-----------+---------+----------+------+------+\n",
            "|     James|           |    Smith|1991-04-01|     M|  3000|\n",
            "|   Michael|       Rose|         |2000-05-19|     M|  4000|\n",
            "|    Robert|           | Williams|1978-09-05|     M|  4000|\n",
            "|     Maria|       Anne|    Jones|1967-12-01|     F|  4000|\n",
            "|       Jen|       Mary|    Brown|1980-02-17|     F|    -1|\n",
            "+----------+-----------+---------+----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now create one column which as 5 times of your existing salary\n",
        "df = df.withColumn(\"FiveTimessalary\", col(\"salary\") * 5)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXk7zkmvFiB5",
        "outputId": "53cd04f9-f251-4601-c744-a3c3086e26ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+---------+----------+------+------+---------------+\n",
            "|first_name|middle_name|last_name|       dob|gender|salary|FiveTimessalary|\n",
            "+----------+-----------+---------+----------+------+------+---------------+\n",
            "|     James|           |    Smith|1991-04-01|     M|  3000|          15000|\n",
            "|   Michael|       Rose|         |2000-05-19|     M|  4000|          20000|\n",
            "|    Robert|           | Williams|1978-09-05|     M|  4000|          20000|\n",
            "|     Maria|       Anne|    Jones|1967-12-01|     F|  4000|          20000|\n",
            "|       Jen|       Mary|    Brown|1980-02-17|     F|    -1|             -5|\n",
            "+----------+-----------+---------+----------+------+------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PYSpark SQL"
      ],
      "metadata": {
        "id": "ztlN5NQhHvvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"Read Top 5 Rows from CSV\").getOrCreate()\n",
        "df = spark.read.csv(\"housing.csv\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sLqqxv9sa1D4",
        "outputId": "7c47589f-40b2-4ac8-cdbf-cb14a8642fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-900298d62315>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read Top 5 Rows from CSV\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"housing.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o384.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 64.0 failed 1 times, most recent failure: Lost task 0.0 in stage 64.0 (TID 50) (e1661b71e150 executor driver): org.apache.spark.SparkException: File /tmp/spark-5df8387f-fad9-411e-80a9-e9317b185438/userFiles-1147b8d4-3504-42c3-8c0d-b30c693084e5/view exists and does not match contents of https://drive.google.com/file/d/1i2w07jdXXLJeSNize4xszDP8GgtSqAar/view\n\tat org.apache.spark.util.Utils$.copyFile(Utils.scala:720)\n\tat org.apache.spark.util.Utils$.downloadFile(Utils.scala:671)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:808)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:555)\n\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$5(Executor.scala:1033)\n\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$5$adapted(Executor.scala:1030)\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)\n\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)\n\tat org.apache.spark.executor.Executor.updateDependencies(Executor.scala:1030)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:511)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: File /tmp/spark-5df8387f-fad9-411e-80a9-e9317b185438/userFiles-1147b8d4-3504-42c3-8c0d-b30c693084e5/view exists and does not match contents of https://drive.google.com/file/d/1i2w07jdXXLJeSNize4xszDP8GgtSqAar/view\n\tat org.apache.spark.util.Utils$.copyFile(Utils.scala:720)\n\tat org.apache.spark.util.Utils$.downloadFile(Utils.scala:671)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:808)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:555)\n\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$5(Executor.scala:1033)\n\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$5$adapted(Executor.scala:1030)\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)\n\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)\n\tat org.apache.spark.executor.Executor.updateDependencies(Executor.scala:1030)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:511)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G2SqvTyOa9OV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}